{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H57yZKsDjfjp",
        "outputId": "b6c55128-21d2-4563-e1d7-1f52bbf83085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10, Average Reward: -202.72, Epsilon: 0.01\n",
            "Episode 20, Average Reward: -173.71, Epsilon: 0.01\n",
            "Episode 30, Average Reward: -162.69, Epsilon: 0.01\n",
            "Episode 40, Average Reward: -158.16, Epsilon: 0.01\n",
            "Episode 50, Average Reward: -164.72, Epsilon: 0.01\n",
            "Episode 60, Average Reward: -162.80, Epsilon: 0.01\n",
            "Episode 70, Average Reward: -160.51, Epsilon: 0.01\n",
            "Episode 80, Average Reward: -162.06, Epsilon: 0.01\n",
            "Episode 90, Average Reward: -166.02, Epsilon: 0.01\n",
            "Episode 100, Average Reward: -158.39, Epsilon: 0.01\n",
            "Episode 110, Average Reward: -170.03, Epsilon: 0.01\n",
            "Episode 120, Average Reward: -156.74, Epsilon: 0.01\n",
            "Episode 130, Average Reward: -165.59, Epsilon: 0.01\n",
            "Episode 140, Average Reward: -158.50, Epsilon: 0.01\n",
            "Episode 150, Average Reward: -163.35, Epsilon: 0.01\n",
            "Episode 160, Average Reward: -167.54, Epsilon: 0.01\n",
            "Episode 170, Average Reward: -166.29, Epsilon: 0.01\n",
            "Episode 180, Average Reward: -169.00, Epsilon: 0.01\n",
            "Episode 190, Average Reward: -167.75, Epsilon: 0.01\n",
            "Episode 200, Average Reward: -158.39, Epsilon: 0.01\n",
            "Episode 210, Average Reward: -159.80, Epsilon: 0.01\n",
            "Episode 220, Average Reward: -167.77, Epsilon: 0.01\n",
            "Episode 230, Average Reward: -160.11, Epsilon: 0.01\n",
            "Episode 240, Average Reward: -161.74, Epsilon: 0.01\n",
            "Episode 250, Average Reward: -166.50, Epsilon: 0.01\n",
            "Episode 260, Average Reward: -168.52, Epsilon: 0.01\n",
            "Episode 270, Average Reward: -169.66, Epsilon: 0.01\n",
            "Episode 280, Average Reward: -161.43, Epsilon: 0.01\n",
            "Episode 290, Average Reward: -168.71, Epsilon: 0.01\n",
            "Episode 300, Average Reward: -168.32, Epsilon: 0.01\n",
            "Episode 310, Average Reward: -162.77, Epsilon: 0.01\n",
            "Episode 320, Average Reward: -167.37, Epsilon: 0.01\n",
            "Episode 330, Average Reward: -163.71, Epsilon: 0.01\n",
            "Episode 340, Average Reward: -169.87, Epsilon: 0.01\n",
            "Episode 350, Average Reward: -164.90, Epsilon: 0.01\n",
            "Episode 360, Average Reward: -160.90, Epsilon: 0.01\n",
            "Episode 370, Average Reward: -173.89, Epsilon: 0.01\n",
            "Episode 380, Average Reward: -167.72, Epsilon: 0.01\n",
            "Episode 390, Average Reward: -175.23, Epsilon: 0.01\n",
            "Episode 400, Average Reward: -165.68, Epsilon: 0.01\n",
            "Episode 410, Average Reward: -181.27, Epsilon: 0.01\n",
            "Episode 420, Average Reward: -169.33, Epsilon: 0.01\n",
            "Episode 430, Average Reward: -168.65, Epsilon: 0.01\n",
            "Episode 440, Average Reward: -169.83, Epsilon: 0.01\n",
            "Episode 450, Average Reward: -165.85, Epsilon: 0.01\n",
            "Episode 460, Average Reward: -162.56, Epsilon: 0.01\n",
            "Episode 470, Average Reward: -166.03, Epsilon: 0.01\n",
            "Episode 480, Average Reward: -172.29, Epsilon: 0.01\n",
            "Episode 490, Average Reward: -159.28, Epsilon: 0.01\n",
            "Episode 500, Average Reward: -166.08, Epsilon: 0.01\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from gym import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "\n",
        "# -----------------------------\n",
        "# Supply Chain Gym Environment\n",
        "# -----------------------------\n",
        "class SupplyChainEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A simple supply chain simulation environment.\n",
        "    At each time step, the agent selects a production multiplier (0, 1, or 2).\n",
        "    The production is subject to random disruption.\n",
        "    Demand is generated with a seasonal pattern.\n",
        "\n",
        "    The state is represented as:\n",
        "      - current inventory level (normalized)\n",
        "      - current seasonal factor (sinusoidal signal)\n",
        "\n",
        "    The reward is defined as the negative total cost incurred:\n",
        "      cost = production_cost * production + holding_cost * new_inventory + shortage_penalty * shortage\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SupplyChainEnv, self).__init__()\n",
        "\n",
        "        # Define action and observation spaces\n",
        "        # Action: 0 (no production), 1 (normal production), 2 (increased production)\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        # Observation: [inventory level, seasonal factor]\n",
        "        self.observation_space = spaces.Box(low=np.array([0.0, -1.0]),\n",
        "                                            high=np.array([100.0, 1.0]),\n",
        "                                            dtype=np.float32)\n",
        "\n",
        "        # Environment parameters\n",
        "        self.baseline_production = 1.0      # base production units\n",
        "        self.production_cost = 0.5          # cost per unit produced\n",
        "        self.holding_cost = 0.05            # cost per unit in inventory per step\n",
        "        self.shortage_penalty = 2.0         # penalty per unit short\n",
        "        self.DISRUPTION_PROB = 0.1          # probability that production is disrupted (production becomes 0)\n",
        "\n",
        "        # Demand generation parameters\n",
        "        self.demand_min = 1\n",
        "        self.demand_max = 3\n",
        "        self.seasonal_variation = 0.3       # amplitude of seasonal effect\n",
        "        self.season_period = 20             # period of the seasonal sine wave\n",
        "\n",
        "        # Episode parameters\n",
        "        self.max_steps = 100\n",
        "        self.current_step = 0\n",
        "        self.inventory = 0.0\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.inventory = 0.0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # Seasonal factor based on current step\n",
        "        seasonal_factor = math.sin(2 * math.pi * self.current_step / self.season_period)\n",
        "        # We can optionally normalize inventory; here we keep it raw\n",
        "        return np.array([self.inventory, seasonal_factor], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Action: production multiplier\n",
        "            0: no production\n",
        "            1: normal production (1 unit)\n",
        "            2: increased production (2 units)\n",
        "        \"\"\"\n",
        "        # Convert action into production target\n",
        "        multiplier = float(action)  # 0, 1, or 2\n",
        "        planned_production = multiplier * self.baseline_production\n",
        "\n",
        "        # Check for production disruption\n",
        "        if random.random() < self.DISRUPTION_PROB:\n",
        "            production = 0.0\n",
        "            disruption = True\n",
        "        else:\n",
        "            production = planned_production\n",
        "            disruption = False\n",
        "\n",
        "        # Generate demand (seasonal effect)\n",
        "        # Seasonal factor (scales demand): 1 + variation * sin(...)\n",
        "        seasonal_effect = 1 + self.seasonal_variation * math.sin(2 * math.pi * self.current_step / self.season_period)\n",
        "        demand = random.randint(self.demand_min, self.demand_max) * seasonal_effect\n",
        "\n",
        "        # Update inventory\n",
        "        available = self.inventory + production\n",
        "        if available >= demand:\n",
        "            shortage = 0.0\n",
        "            fulfilled = demand\n",
        "            new_inventory = available - demand\n",
        "        else:\n",
        "            shortage = demand - available\n",
        "            fulfilled = available\n",
        "            new_inventory = 0.0\n",
        "\n",
        "        # Compute cost\n",
        "        cost = self.production_cost * production + self.holding_cost * new_inventory + self.shortage_penalty * shortage\n",
        "        reward = -cost  # Our goal is to minimize cost\n",
        "\n",
        "        # Update state variables\n",
        "        self.inventory = new_inventory\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps\n",
        "\n",
        "        # Optional: info dict for debugging\n",
        "        info = {\n",
        "            'production': production,\n",
        "            'planned_production': planned_production,\n",
        "            'disruption': disruption,\n",
        "            'demand': demand,\n",
        "            'shortage': shortage,\n",
        "            'inventory': self.inventory,\n",
        "            'cost': cost\n",
        "        }\n",
        "\n",
        "        return self._get_obs(), reward, done, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"Step: {self.current_step}, Inventory: {self.inventory:.2f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Deep Q-Network (DQN) Agent\n",
        "# -----------------------------\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.net(state)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "        else:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.policy_net(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "    def update(self, batch_size):\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
        "\n",
        "        # Compute current Q values\n",
        "        current_q = self.policy_net(states).gather(1, actions)\n",
        "\n",
        "        # Compute next Q values from target network\n",
        "        with torch.no_grad():\n",
        "            next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "\n",
        "        # Compute target Q value\n",
        "        target_q = rewards + self.gamma * next_q * (1 - dones)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = nn.MSELoss()(current_q, target_q)\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Decay epsilon\n",
        "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_end)\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "# -----------------------------\n",
        "# Training Loop\n",
        "# -----------------------------\n",
        "def train(num_episodes=500, batch_size=64, target_update=10):\n",
        "    env = SupplyChainEnv()\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    agent = DQNAgent(state_dim, action_dim)\n",
        "\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0.0\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "\n",
        "            agent.update(batch_size)\n",
        "\n",
        "        # Update target network periodically\n",
        "        if episode % target_update == 0:\n",
        "            agent.update_target()\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-10:])\n",
        "            print(f\"Episode {episode+1}, Average Reward: {avg_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
        "\n",
        "    return agent, episode_rewards\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trained_agent, rewards = train()\n"
      ]
    }
  ]
}